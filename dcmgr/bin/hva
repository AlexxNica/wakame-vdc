#!/usr/bin/env ruby
# -*- coding: utf-8 -*-

begin
  require 'rubygems'
  require 'bundler'
  Bundler.setup(:default)
rescue LoadError
end

require File.expand_path('../../config/path_resolver', __FILE__)

include Isono::Runner::RpcServer
require 'fileutils'

class ServiceNetfilter < Isono::NodeModules::Base
  include Isono::Logger

  initialize_hook do
    myinstance.init_netfilter

    event = Isono::NodeModules::EventChannel.new(node)

    event.subscribe('hva/instance_started', '#') do |args|
      puts "refresh on instance_started: #{args.inspect}"
      myinstance.refresh_netfilter_by_instance_id(args)
    end

    event.subscribe('hva/instance_terminated', '#') do |args|
      puts "refresh on instance_terminated: #{args.inspect}"
      myinstance.refresh_netfilter_by_instance_id(args)
    end

    event.subscribe('hva/netfilter_updated', '#') do |args|
      puts "refresh on netfilter_updated: #{args.inspect}"
      myinstance.refresh_netfilter_by_netfilter_group_id(args)
    end
  end

  def init_netfilter
    EM.defer {
      begin
        init_ebtables
        logger.info("initialize netfilter")
      rescue Exception => e
        p e
      end
    }
  end

  def refresh_netfilter_by_instance_id(args)
    inst_id = args[0]
    raise "UnknownInstanceID" if inst_id.nil?

    EM.defer {
      begin
        refresh_ebtables(inst_id)
        logger.info("refreshed netfilter")
      rescue Exception => e
        p e
      end
    }
  end

  def refresh_netfilter_by_netfilter_group_id(args)
    netfilter_group_id = args[0]
    raise "UnknownNetfilterGroupID" if netfilter_group_id.nil?

    EM.defer {
      begin
        inst_ids = rpc.request('hva-collector', 'get_instances_of_netfilter_group', netfilter_group_id)
        inst_ids.each { |inst_id|
          refresh_ebtables(inst_id) unless inst_id.nil?
        }
      rescue Exception => e
        p e
      end
    }
  end

  def init_ebtables
    cmd = "sudo ebtables --init-table"
    puts cmd
    system(cmd)
  end

  def refresh_ebtables(inst_id)
    p inst = rpc.request('hva-collector', 'get_instance', inst_id)
    raise "UnknownInstanceID" if inst.nil?

    # Does the hva have instance?
    unless inst[:host_pool][:node_id] == node.node_id
      puts "no match for the instance: #{inst_id}"
      return
    end

    network_id = inst[:host_pool][:network_id]
    p network = rpc.request('hva-collector', 'get_network', network_id)
    raise "UnknownNetworkId" if network.nil?

    vif     = inst[:instance_nics].first[:vif]
    vif_mac = inst[:instance_nics].first[:mac_addr].unpack('A2'*6).join(':')
    ng      = rpc.request('hva-collector', 'get_netfilter_groups_of_instance', inst_id)

    rules = ng.map { |g|
      g[:rules].map { |rule| rule[:permission] }
    }
    rules.flatten! if rules.size > 0

    # group node IPv4 addresses.
    ipv4s = rpc.request('hva-collector', 'get_group_instance_ipv4s', inst_id)

    # xtables commands
    cmds = []

    # support IP protocol
    protocol_maps = {
      'ip4'  => 'ip4',
      'arp'  => 'arp',
      #ip6'  => 'ip6',
      #rarp' => '0x8035',
    }

    # make chain names.
    chains = []
    chains << "s_#{vif}"
    chains << "d_#{vif}"
    chains << "s_#{vif}_d_host"
    protocol_maps.each { |k,v|
      chains << "s_#{vif}_#{k}"
      chains << "d_#{vif}_#{k}"
      chains << "s_#{vif}_d_host_#{k}"
    }

    # clear rules if exists.
    system("sudo ebtables -L s_#{vif} >/dev/null 2>&1")
    if $?.exitstatus == 0
      system("sudo ebtables -D FORWARD -i #{vif} -j s_#{vif}")
    end

    system("sudo ebtables -L d_#{vif} >/dev/null 2>&1")
    if $?.exitstatus == 0
      system("sudo ebtables -D FORWARD -o #{vif} -j d_#{vif}")
    end

    system("sudo ebtables -L s_#{vif}_d_host >/dev/null 2>&1")
    if $?.exitstatus == 0
      system("sudo ebtables -D INPUT -i #{vif} -j s_#{vif}_d_host")
    end

    [ 'F', 'X' ].each { |cmd|
      chains.each { |chain|
        system("sudo ebtables -L #{chain} >/dev/null 2>&1")
        if $?.exitstatus == 0
          system("sudo ebtables -#{cmd} #{chain}")
        end
      }
    }

    # create user defined chains.
    [ 'N' ].each { |cmd|
      chains.each { |chain|
        cmds << "sudo ebtables -#{cmd} #{chain}"
      }
    }

    # jumt to user defined chains
    cmds << "sudo ebtables -A FORWARD -i #{vif} -j s_#{vif}"
    cmds << "sudo ebtables -A FORWARD -o #{vif} -j d_#{vif}"
    cmds << "sudo ebtables -A INPUT   -i #{vif} -j s_#{vif}_d_host"

    # IP protocol routing
    protocol_maps.each { |k,v|
      cmds << "sudo ebtables -A s_#{vif}        -p #{v} -j s_#{vif}_#{k}"
      cmds << "sudo ebtables -A d_#{vif}        -p #{v} -j d_#{vif}_#{k}"
      cmds << "sudo ebtables -A s_#{vif}_d_host -p #{v} -j s_#{vif}_d_host_#{k}"
    }

    # default drop
    cmds << "sudo ebtables -A s_#{vif}        -j DROP"
    cmds << "sudo ebtables -A s_#{vif}_d_host -j DROP"

    # anti spoof
    cmds << "sudo ebtables -A s_#{vif}_arp --protocol arp --arp-mac-src ! #{vif_mac} -j DROP"
    cmds << "sudo ebtables -A d_#{vif}_arp --protocol arp --arp-mac-dst ! #{vif_mac} -j DROP"

    # group nodes.
    # [ToDo] DHCP Server
    ipv4s << network[:ipv4_gw]
    ipv4s << network[:dns_server]
    ipv4s.uniq.each do |ipv4|
      cmds << "sudo ebtables -A s_#{vif}_arp --protocol arp --arp-ip-src #{ipv4} -j ACCEPT"
    end

    # deny,allow
    cmds << "sudo ebtables -A d_#{vif}_arp        -j DROP"
    cmds << "sudo ebtables -A s_#{vif}_d_host_arp -j DROP"

    cmds.uniq! if cmds.size > 0
    cmds.compact.each { |cmd|
      puts cmd
      system(cmd)
    }
  end

  def build_rule(rules = [])
    rule_maps = []

    rules.each do |rule|
      # ex. "tcp:80,80,ip4:0.0.0.0"
      from_pair, ip_dport, source_pair = rule.split(',')

      ip_protocol, ip_sport = from_pair.split(':')
      protocol, ip_source   = source_pair.split(':')

      case ip_protocol
      when 'tcp'
        rule_maps << {
          :protocol => protocol,
          :ip_source => ip_source,
          :ip_protocol => ip_protocol,
          :ip_dport => ip_dport,
        }
      when 'udp'
        rule_maps << {
          :protocol => protocol,
          :ip_source => ip_source,
          :ip_protocol => ip_protocol,
          :ip_dport => ip_dport,
        }
      when 'icmp'
        # through, pending
      end
    end

    rule_maps
  end

  def rpc
    @rpc ||= Isono::NodeModules::RpcChannel.new(@node)
  end

  def event
    @event ||= Isono::NodeModules::EventChannel.new(@node)
  end

end


class KvmHandler < EndpointBuilder
  include Isono::Logger

  job :run_local_store do
    #hva = rpc.delegate('hva-collector')
    inst_id = request.args[0]
    logger.info("Booting #{inst_id}")
    #inst = hva.get_instance(inst_id)

    inst = rpc.request('hva-collector', 'get_instance',  inst_id)
    raise "Invalid instance state: #{inst[:state]}" unless inst[:state].to_s == 'init'

    # setup vm data folder
    inst_data_dir = File.expand_path("#{inst_id}", @node.manifest.config.vm_data_dir)
    FileUtils.mkdir(inst_data_dir)
    # copy image file
    img_src = inst[:image][:source]
    case img_src[:type].to_sym
    when :http
      img_path = File.expand_path("#{inst_id}/#{inst[:uuid]}", @node.manifest.config.vm_data_dir)
      system("curl --silent -o '#{img_path}' #{img_src[:uri]}")
    else
      raise "Unknown image source type: #{img_src[:type]}"
    end

    # boot virtual machine
    cmd = sprintf("kvm -m %d -smp %d -name vdc-%s -vnc :%d -drive file=%s -pidfile %s -daemonize -monitor telnet::%d,server,nowait",
                  inst[:instance_spec][:memory_size],
                  inst[:instance_spec][:cpu_cores],
                  inst_id,
                  inst[:runtime_config][:vnc_port],
                  img_path,
                  File.expand_path('kvm.pid', inst_data_dir),
                  inst[:runtime_config][:telnet_port]
                  )
    system(cmd)
    
    rpc.request('hva-collector', 'update_instance',  inst_id, {:state=>:running})
    event.publish('hva/instance_started', :args=>[inst_id])
  end

  job :run_vol_store do
    inst_id = request.args[0]
    vol_id = request.args[1]
    
    inst = rpc.request('hva-collector', 'get_instance', inst_id)
    vol = rpc.request('sta-collector', 'get_volume', vol_id)
    logger.info("Booting #{inst_id}")
    raise "Invalid instance state: #{inst[:state]}" unless inst[:state].to_s == 'init'

    # setup vm data folder
    inst_data_dir = File.expand_path("#{inst_id}", @node.manifest.config.vm_data_dir)
    FileUtils.mkdir(inst_data_dir)
    
    # create volume from snapshot
    jobreq.run("sta-loader.#{vol[:storage_pool][:node_id]}", "create", vol_id)
    
    puts "volume created on #{vol[:storage_pool][:node_id]}: #{vol_id}"
    # reload volume info
    vol = rpc.request('sta-collector', 'get_volume', vol_id)

    # check under until the dev file is created.
    # /dev/disk/by-path/ip-192.168.1.21:3260-iscsi-iqn.1986-03.com.sun:02:a1024afa-775b-65cf-b5b0-aa17f3476bfc-lun-0
    linux_dev_path = "/dev/disk/by-path/ip-%s-iscsi-%s-lun-%d" % ["#{vol[:storage_pool][:ipaddr]}:3260",
                                                                  vol[:transport_information][:iqn],
                                                                  vol[:transport_information][:lun]]
    
    # attach disk
    tryagain do
      lists = `sudo iscsiadm -m discovery -t sendtargets -p #{vol[:storage_pool][:ipaddr]}`
      initiator = `sudo iscsiadm -m node -l -T '#{vol[:transport_information][:iqn]}' --portal '#{vol[:storage_pool][:ipaddr]}:3260'`
      sleep 1
      File.exist?(linux_dev_path)
    end
    
    # run vm
    cmd = sprintf("kvm -m %d -smp %d -name vdc-%s -vnc :%d -drive file=%s -pidfile %s -daemonize -monitor telnet::%d,server,nowait",
                  inst[:instance_spec][:memory_size],
                  inst[:instance_spec][:cpu_cores],
                  inst_id,
                  inst[:runtime_config][:vnc_port],
                  linux_dev_path,
                  File.expand_path('kvm.pid', inst_data_dir),
                  inst[:runtime_config][:telnet_port]
                  )
    if vnic = inst[:instance_nics].first
      cmd += " -net nic,macaddr=#{vnic[:mac_addr].unpack('A2'*6).join(':')} -net tap,ifname=#{vnic[:vif]}"
    end
    puts  cmd
    system(cmd)

    rpc.request('hva-collector', 'update_instance',  inst_id, {:state=>:running})
    event.publish('hva/instance_started', :args=>[inst_id])
  end
  
  job :terminate do
    inst_id = request.args[0]

    inst = rpc.request('hva-collector', 'get_instance', inst_id)
    raise "Invalid instance state: #{inst[:state]}" unless inst[:state].to_s == 'running'

    rpc.request('hva-collector', 'update_instance',  inst_id, {:state=>:shuttingdown})
    
    kvm_pid=`pgrep -u root -f vdc-#{inst_id}`
    unless $?.exitstatus == 0 && kvm_pid.to_s =~ /^\d+$/
      raise "No such VM process: kvm -name vdc-#{inst_id}"
    end
    
    system("/bin/kill #{kvm_pid}")

    unless inst[:volume].nil?
      inst[:volume].each { |volid, v|
        `sudo iscsiadm -m node -T '#{v[:transport_information][:iqn]}' --logout`
      }
    end

    # cleanup vm data folder
    FileUtils.rm_r(File.expand_path("#{inst_id}", @node.manifest.config.vm_data_dir))
    
    rpc.request('hva-collector', 'update_instance',  inst_id, {:state=>:terminated})
    event.publish('hva/instance_terminated', :args=>[inst_id])
  end

  job :attach do
    inst_id = request.args[0]
    vol_id = request.args[1]

    job = Dcmgr::Stm::VolumeContext.new(vol_id)
    inst = rpc.request('hva-collector', 'get_instance', inst_id)
    vol = rpc.request('sta-collector', 'get_volume', vol_id)
    puts "Attaching #{vol_id}"
    job.stm.state = vol[:state].to_sym
    raise "Invalid volume state: #{vol[:state]}" unless vol[:state].to_s == 'available'

    job.stm.on_attach
    # check under until the dev file is created.
    # /dev/disk/by-path/ip-192.168.1.21:3260-iscsi-iqn.1986-03.com.sun:02:a1024afa-775b-65cf-b5b0-aa17f3476bfc-lun-0
    linux_dev_path = "/dev/disk/by-path/ip-%s-iscsi-%s-lun-%d" % ["#{vol[:storage_pool][:ipaddr]}:3260",
                                                                  vol[:transport_information][:iqn],
                                                                  vol[:transport_information][:lun]]

    # attach disk on host os
    tryagain do
      lists = `sudo iscsiadm -m discovery -t sendtargets -p #{vol[:storage_pool][:ipaddr]}`
      initiator = `sudo iscsiadm -m node -l -T '#{vol[:transport_information][:iqn]}' --portal '#{vol[:storage_pool][:ipaddr]}:3260'`
      sleep 1
      File.exist?(linux_dev_path)
    end

    rpc.request('sta-collector', 'update_volume', job.to_hash(:host_device_name => linux_dev_path))
    puts "Attaching #{vol_id} on #{inst_id}"
    job.stm.on_attach
    job.on_attach

    # attach disk on guest os
    require 'net/telnet'
    slot_number = nil
    pci = nil

    slink = `ls -la #{linux_dev_path}`.scan(/.+\s..\/..\/([a-z]+)/)
    raise "volume has not attached host os" if slink.nil?

    begin
      telnet = ::Net::Telnet.new("Host" => "localhost", "Port"=>"#{inst[:runtime_config][:telnet_port]}", "Prompt" => /\n\(qemu\) /, "Timeout" => 60, "Waittime" => 0.2)
      telnet.cmd({"String" => "pci_add auto storage file=/dev/#{slink},if=scsi", "Match" => /.+slot\s[0-9]+.+/}){|c|
        pci_add = c.scan(/.+slot\s([0-9]+).+/)
        slot_number = pci_add unless pci_add.empty?
      }
      telnet.cmd("info pci"){|c|
        pci = c.scan(/^(.+[a-zA-z]+.+[0-9],.+device.+#{slot_number},.+:)/)
      }
    rescue => e
      puts e
    ensure
      telnet.close
    end
    raise "volume has not attached" if pci.nil?
    rpc.request('sta-collector', 'update_volume', job.to_hash(:guest_device_name=>slot_number))
    puts "Attached #{vol_id} on #{inst_id}"
  end

  job :detach do
    inst_id = request.args[0]
    vol_id = request.args[1]

    job = Dcmgr::Stm::VolumeContext.new(vol_id)
    inst = rpc.request('hva-collector', 'get_instance', inst_id)
    vol = rpc.request('sta-collector', 'get_volume', vol_id)
    puts "Detaching #{vol_id} on #{inst_id}"
    job.stm.state = vol[:state].to_sym
    raise "Invalid volume state: #{vol[:state]}" unless vol[:state].to_s == 'attached'

    job.stm.on_detach
    # detach disk on guest os
    require 'net/telnet'
    pci = nil

    begin
      telnet = ::Net::Telnet.new("Host" => "localhost", "Port"=>"#{inst[:runtime_config][:telnet_port]}", "Prompt" => /\n\(qemu\) /, "Timeout" => 60, "Waittime" => 0.2)
      telnet.cmd("pci_del #{vol[:guest_device_name]}")
      telnet.cmd("info pci"){|c|
        pci = c.scan(/^(.+[a-zA-z]+.+[0-9],.+device.+#{vol[:guest_device_name]},.+:)/)
      }
    rescue => e
      puts e
    ensure
      telnet.close
    end
    raise "volume has not detached" unless pci.empty?
    rpc.request('sta-collector', 'update_volume', job.to_hash)

    # iscsi logout
    job.stm.on_detach
    job.on_detach
    puts "iscsi logout #{vol_id}: #{vol[:transport_information][:iqn]}"
    initiator = `sudo iscsiadm -m node -T '#{vol[:transport_information][:iqn]}' --logout`
    rpc.request('sta-collector', 'update_volume', job.to_hash)
  end

  def rpc
    @rpc ||= Isono::NodeModules::RpcChannel.new(@node)
  end

  def jobreq
    @jobreq ||= Isono::NodeModules::JobChannel.new(@node)
  end

  def event
    @event ||= Isono::NodeModules::EventChannel.new(@node)
  end


  class TimeoutError < RuntimeError; end
  
  def tryagain(opts={:timeout=>60, :retry=>3}, &blk)
    timedout = false
    curthread = Thread.current

    timersig = EventMachine.add_timer(opts[:timeout]) {
      timedout = true
      if curthread
        curthread.raise(TimeoutError.new("timeout"))
        curthread.pass
      end
    }

    begin
      count = 0
      begin
        break if blk.call
      end while !timedout && ((count += 1) < opts[:retry])
    rescue TimeoutError => e
      raise e
    ensure
      curthread = nil
      EventMachine.cancel_timer(timersig) rescue nil
    end
  end

end


manifest = DEFAULT_MANIFEST.dup
manifest.instance_eval do
  node_name 'hva'
  node_instance_id "#{Isono::Util.default_gw_ipaddr}"
  load_module Isono::NodeModules::NodeHeartbeat
  load_module ServiceNetfilter

  config do |c|
    c.vm_data_dir = '/var/lib/vm'
  end

  config_path File.expand_path('config/hva.conf', app_root)
  load_config
end

start(manifest) do
  endpoint "kvm-handle.#{@node.node_id}", KvmHandler
end
