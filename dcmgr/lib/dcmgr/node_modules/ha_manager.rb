# -*- coding: utf-8 -*-
require 'isono'

module Dcmgr
  module NodeModules
    class HaManager < Isono::NodeModules::Base
      include Dcmgr::Logger

      initialize_hook do
        @thread_pool = Isono::ThreadPool.new(1, 'HA_Monitor')
        event = Isono::NodeModules::EventChannel.new(node)
        event.subscribe('hva/fault_instance', '#') { |args|
          @thread_pool.pass {
            inst_id = args[0]

            inst = rpc.request('hva-collector', 'get_instance', inst_id)
            # Here may get the event from the instances that no longer
            # exits on the database.
            if inst.nil?
              logger.error("Received fault instance event from unknown instance: #{inst_id}")
              next
            end

            # check if the instance has HA enable.
            if inst.ha_enabled == 0
              next
            else
              myinstance.restart_instance(inst)
            end
          }
        }

        # HostNode failure detection
        if Dcmgr.conf.instance_ha.monitor_script_path
          EM.add_periodic_timer(Dcmgr.conf.instance_ha.monitor_frequency_sec) {
            @thread_pool.pass {
              myinstance.monitor_host_node_external
            }
          }
        end
      end

      terminate_hook do
        @thread_pool.shutdown
      end

      # Detects host node failure by checking external monitoring
      # script and trigger evacuation process for instances with HA
      # enabled.
      #
      # monitoring script is expected to return exit code as follows:
      #  success: 0
      #  detected failure: 100
      #  other failure: !0 && !100
      def monitor_host_node_external
        host_node_lst = rpc.request('hva-collector', 'get_external_monitor_target_host_nodes')
        host_node_lst.each { |host_uuid|
          system("#{Dcmgr.conf.instance_ha.monitor_script_path} #{host_uuid}")
          case $?.exitstatus
          when 0
            logger.debug("no failure in #{host_uuid}.")
          when 100
            logger.error("detected #{host_uuid} failure.")
            jobrpc.submit("scheduler", 'evacuate_from',
                          host_uuid, true)
          else
            logger.warn("monitor script is getting something failed.")
          end
        }
      end

      def update_instance_state(inst, opts, ev)
        rpc.request('hva-collector', 'update_instance', inst.canonical_uuid, opts)
        event.publish(ev, :args=>[inst.canonical_uuid])
      end

      def restart_instance(inst)
        # terminate and cleanup

        # set boot_vol before instance is cleaned up.
        case inst.image.boot_dev_type
        when Models::Image::BOOT_DEV_SAN
          boot_vol = inst.volume.find {|v| v.boot_dev == 1 }
        end

        begin
          jobrpc.run("hva-handle.#{inst.host_node.node_id}", 'cleanup', inst.canonical_uuid)
        rescue => e
          # instance termination may fail
        end

        # TODO: pick a new host node
        Isono::NodeModules::DataStore.barrier {
          inst.state = :failingover
          inst.save_changes
        }

        # schedule the location for new instance.
        case inst.image.boot_dev_type
        when Models::Image::BOOT_DEV_SAN
          jobrpc.submit("scheduler", 'schedule_instance_ha', inst.canonical_uuid, boot_vol)
        when Models::Image::BOOT_DEV_LOCAL
          jobrpc.submit("scheduler", 'schedule_instance_ha', inst.canonical_uuid, nil)
        else
          raise "Unknown boot type"
        end

        logger.info("#{inst.canonical_uuid} has been restarted")
      end

      private
      def event
        @event ||= Isono::NodeModules::EventChannel.new(node)
      end

      def jobrpc
        @jobrpc ||= Isono::NodeModules::JobChannel.new(node)
      end

      def rpc
        @rpc ||= Isono::NodeModules::RpcChannel.new(@node)
      end
    end
  end
end
