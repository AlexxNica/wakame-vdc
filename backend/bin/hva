#!/usr/bin/env ruby
# -*- coding: utf-8 -*-

require File.expand_path('../../config/path_resolver', __FILE__)

include Isono::Runner::RpcServer
require 'fileutils'

require 'stm/volume_context'

class ServiceNetfilter < Isono::NodeModules::Base
  include Isono::Logger

  initialize_hook do
    myinstance.init_netfilter

    event = Isono::NodeModules::EventChannel.new(node)

    event.subscribe('hva/instance_started', '#') do |args|
      puts "refresh on instance_started: #{args.inspect}"
      myinstance.refresh_netfilter_by_instance_id(args)
    end

    event.subscribe('hva/instance_terminated', '#') do |args|
      puts "refresh on instance_terminated: #{args.inspect}"
      myinstance.refresh_netfilter_by_instance_id(args)
    end

    event.subscribe('hva/netfilter_updated', '#') do |args|
      puts "refresh on netfilter_updated: #{args.inspect}"
      myinstance.refresh_netfilter_by_netfilter_group_id(args)
    end
  end

  def init_netfilter
    EM.defer {
      begin
        init_ebtables
        logger.info("initialize netfilter")
      rescue Exception => e
        p e
      end
    }
  end

  def refresh_netfilter_by_instance_id(args)
    inst_id = args[0]
    raise "UnknownInstanceID" if inst_id.nil?

    EM.defer {
      begin
        refresh_ebtables(inst_id)
        logger.info("refreshed netfilter")
      rescue Exception => e
        p e
      end
    }
  end

  def refresh_netfilter_by_netfilter_group_id(args)
    netfilter_group_id = args[0]
    raise "UnknownNetfilterGroupID" if netfilter_group_id.nil?

    EM.defer {
      begin
        inst_ids = rpc.request('hva-collector', 'get_instances_of_netfilter_group', netfilter_group_id)
        inst_ids.each { |inst_id|
          refresh_ebtables(inst_id) unless inst_id.nil?
        }
      rescue Exception => e
        p e
      end
    }
  end

  def init_ebtables
    cmd = "sudo ebtables --init-table"
    puts cmd
    system(cmd)
  end

  def refresh_ebtables(inst_id)
    p inst = rpc.request('hva-collector', 'get_instance', inst_id)
    raise "UnknownInstanceID" if inst.nil?

    # Does the hva have instance?
    unless inst[:host_pool][:node_id] == node.node_id
      puts "no match for the instance: #{inst_id}"
      return
    end

    vif = inst[:instance_nics].first[:vif]
    ng = rpc.request('hva-collector', 'get_netfilter_groups_of_instance', inst_id)

    rules = ng.map { |g|
      g[:rules].map { |rule| rule[:permission] }
    }
    rules.flatten! if rules.size > 0

    # group ips
    ipv4s = rpc.request('hva-collector', 'get_group_instance_ipv4s', inst_id)

    # ebtables
    ebtables = []

    # chains
    chains = []
    protocol_maps = {
      'ip4'  => 'ip4',
      'arp'  => 'arp',
      #ip6'  => 'ip6',
      #rarp' => '0x8035',
    }

    # make chain names.
    chains << "host_I_#{vif}"
    protocol_maps.each { |k,v|
      chains << "I_#{vif}_#{k}"
    }

    ebtables << "-D FORWARD -o #{vif} -j host_I_#{vif}"
    [ 'F', 'X', 'N', 'Z' ].each { |cmd|
      chains.each { |chain|
        ebtables << "-#{cmd} #{chain}"
      }
    }

    # chain for VIF
    ebtables << "-A FORWARD -o #{vif} -j host_I_#{vif}"
    protocol_maps.each { |k,v|
      ebtables << "-A host_I_#{vif} -p #{v} -j I_#{vif}_#{k}"
    }
    ebtables << "-A host_I_#{vif} -j DROP"

    # rules
    self.build_rule(rules).each do |rule|
      case rule[:ip_source]
      when '0.0.0.0'
        ebtables << "-A I_#{vif}_#{rule[:protocol]} --protocol #{rule[:protocol]} --ip-protocol #{rule[:ip_protocol]} --ip-dport #{rule[:ip_dport]} -j ACCEPT"
      else
        ebtables << "-A I_#{vif}_#{rule[:protocol]} --protocol #{rule[:protocol]} --ip-protocol #{rule[:ip_protocol]} --ip-source #{rule[:ip_source]} --ip-dport #{rule[:ip_dport]} -j ACCEPT"
      end
    end

    # group instances
    ipv4s.each do |ipv4|
      ebtables << "-A I_#{vif}_ip4 --protocol ip4 --ip-source  #{ipv4} -j ACCEPT"
      ebtables << "-A I_#{vif}_ip4 --protocol arp --arp-ip-src #{ipv4} -j ACCEPT"
    end

    # dhcp server
    ebtables << "-A I_#{vif}_ip4 --protocol ip4 --ip-proto udp --ip-sport 67 --ip-dport 68 -j ACCEPT"

    protocol_maps.each { |k,v|
      ebtables << "-A I_#{vif}_#{k} -j DROP"
    }

    ebtables.uniq!
    ebtables.each { |ebtable|
      cmd = "sudo ebtables #{ebtable}"
      puts cmd
      system(cmd)
    }
  end

  def build_rule(rules = [])
    rule_maps = []

    rules.each do |rule|
      # ex. "tcp:80,80,ip4:0.0.0.0"
      from_pair, ip_dport, source_pair = rule.split(',')

      ip_protocol, ip_sport = from_pair.split(':')
      protocol, ip_source   = source_pair.split(':')

      case ip_protocol
      when 'tcp'
        rule_maps << {
          :protocol => protocol,
          :ip_source => ip_source,
          :ip_protocol => ip_protocol,
          :ip_dport => ip_dport,
        }
      when 'udp'
        rule_maps << {
          :protocol => protocol,
          :ip_source => ip_source,
          :ip_protocol => ip_protocol,
          :ip_dport => ip_dport,
        }
      when 'icmp'
        # through, pending
      end
    end

    rule_maps
  end

  def rpc
    @rpc ||= Isono::NodeModules::RpcChannel.new(@node)
  end

  def event
    @event ||= Isono::NodeModules::EventChannel.new(@node)
  end

end


class KvmHandler < EndpointBuilder
  include Isono::Logger

  job :run_local_store do
    #hva = rpc.delegate('hva-collector')
    inst_id = request.args[0]
    logger.info("Booting #{inst_id}")
    #inst = hva.get_instance(inst_id)

    inst = rpc.request('hva-collector', 'get_instance',  inst_id)
    raise "Invalid instance state: #{inst[:state]}" unless inst[:state].to_s == 'init'

    # setup vm data folder
    inst_data_dir = File.expand_path("#{inst_id}", @node.manifest.config.vm_data_dir)
    FileUtils.mkdir(inst_data_dir)
    # copy image file
    img_src = inst[:image][:source]
    case img_src[:type].to_sym
    when :http
      img_path = File.expand_path("#{inst_id}/#{inst[:uuid]}", @node.manifest.config.vm_data_dir)
      system("curl --silent -o '#{img_path}' #{img_src[:uri]}")
    else
      raise "Unknown image source type: #{img_src[:type]}"
    end

    # boot virtual machine
    cmd = sprintf("kvm -m %d -smp %d -name vdc-%s -vnc :%d -drive file=%s -pidfile %s -daemonize -monitor telnet::%d,server,nowait",
                  inst[:instance_spec][:memory_size],
                  inst[:instance_spec][:cpu_cores],
                  inst_id,
                  inst[:runtime_config][:vnc_port],
                  img_path,
                  File.expand_path('kvm.pid', inst_data_dir),
                  inst[:runtime_config][:telnet_port]
                  )
    system(cmd)
    
    rpc.request('hva-collector', 'update_instance',  inst_id, {:state=>:running})
    event.publish('hva/instance_started', :args=>[inst_id])
  end

  job :run_vol_store do
    inst_id = request.args[0]
    vol_id = request.args[1]
    
    inst = rpc.request('hva-collector', 'get_instance', inst_id)
    vol = rpc.request('sta-collector', 'get_volume', vol_id)
    logger.info("Booting #{inst_id}")
    raise "Invalid instance state: #{inst[:state]}" unless inst[:state].to_s == 'init'

    # setup vm data folder
    inst_data_dir = File.expand_path("#{inst_id}", @node.manifest.config.vm_data_dir)
    FileUtils.mkdir(inst_data_dir)
    
    # create volume from snapshot
    jobreq.run("sta-loader.#{vol[:storage_pool][:node_id]}", "create", vol_id)
    
    puts "volume created on #{vol[:storage_pool][:node_id]}: #{vol_id}"
    # reload volume info
    vol = rpc.request('sta-collector', 'get_volume', vol_id)

    # check under until the dev file is created.
    # /dev/disk/by-path/ip-192.168.1.21:3260-iscsi-iqn.1986-03.com.sun:02:a1024afa-775b-65cf-b5b0-aa17f3476bfc-lun-0
    linux_dev_path = "/dev/disk/by-path/ip-%s-iscsi-%s-lun-%d" % ["#{vol[:storage_pool][:ipaddr]}:3260",
                                                                  vol[:transport_information][:iqn],
                                                                  vol[:transport_information][:lun]]
    
    # attach disk
    tryagain do
      lists = `sudo iscsiadm -m discovery -t sendtargets -p #{vol[:storage_pool][:ipaddr]}`
      initiator = `sudo iscsiadm -m node -l -T '#{vol[:transport_information][:iqn]}' --portal '#{vol[:storage_pool][:ipaddr]}:3260'`
      sleep 1
      File.exist?(linux_dev_path)
    end
    
    # run vm
    cmd = sprintf("kvm -m %d -smp %d -name vdc-%s -vnc :%d -drive file=%s -pidfile %s -daemonize -monitor telnet::%d,server,nowait",
                  inst[:instance_spec][:memory_size],
                  inst[:instance_spec][:cpu_cores],
                  inst_id,
                  inst[:runtime_config][:vnc_port],
                  linux_dev_path,
                  File.expand_path('kvm.pid', inst_data_dir),
                  inst[:runtime_config][:telnet_port]
                  )
    if vnic = inst[:instance_nics].first
      cmd += " -net nic,macaddr=#{vnic[:mac_addr].unpack('A2'*6).join(':')} -net tap,ifname=#{vnic[:vif]}"
    end
    puts  cmd
    system(cmd)

    rpc.request('hva-collector', 'update_instance',  inst_id, {:state=>:running})
    event.publish('hva/instance_started', :args=>[inst_id])
  end
  
  job :terminate do
    inst_id = request.args[0]

    inst = rpc.request('hva-collector', 'get_instance', inst_id)
    raise "Invalid instance state: #{inst[:state]}" unless inst[:state].to_s == 'running'

    rpc.request('hva-collector', 'update_instance',  inst_id, {:state=>:shuttingdown})
    
    kvm_pid=`pgrep -u root -f vdc-#{inst_id}`
    unless $?.exitstatus == 0 && kvm_pid.to_s =~ /^\d+$/
      raise "No such VM process: kvm -name vdc-#{inst_id}"
    end
    
    system("/bin/kill #{kvm_pid}")

    unless inst[:volume].nil?
      inst[:volume].each { |volid, v|
        `sudo iscsiadm -m node -T '#{v[:transport_information][:iqn]}' --logout`
      }
    end

    # cleanup vm data folder
    FileUtils.rm_r(File.expand_path("#{inst_id}", @node.manifest.config.vm_data_dir))
    
    rpc.request('hva-collector', 'update_instance',  inst_id, {:state=>:terminated})
    event.publish('hva/instance_terminated', :args=>[inst_id])
  end

  job :attach do
    inst_id = request.args[0]
    vol_id = request.args[1]

    job = Stm::VolumeContext.new(vol_id)
    inst = rpc.request('hva-collector', 'get_instance', inst_id)
    vol = rpc.request('sta-collector', 'get_volume', vol_id)
    puts "Attaching #{vol_id}"
    job.stm.state = vol[:state].to_sym
    raise "Invalid volume state: #{vol[:state]}" unless vol[:state].to_s == 'available'

    job.stm.on_attach
    # check under until the dev file is created.
    # /dev/disk/by-path/ip-192.168.1.21:3260-iscsi-iqn.1986-03.com.sun:02:a1024afa-775b-65cf-b5b0-aa17f3476bfc-lun-0
    linux_dev_path = "/dev/disk/by-path/ip-%s-iscsi-%s-lun-%d" % ["#{vol[:storage_pool][:ipaddr]}:3260",
                                                                  vol[:transport_information][:iqn],
                                                                  vol[:transport_information][:lun]]

    # attach disk on host os
    tryagain do
      lists = `sudo iscsiadm -m discovery -t sendtargets -p #{vol[:storage_pool][:ipaddr]}`
      initiator = `sudo iscsiadm -m node -l -T '#{vol[:transport_information][:iqn]}' --portal '#{vol[:storage_pool][:ipaddr]}:3260'`
      sleep 1
      File.exist?(linux_dev_path)
    end

    rpc.request('sta-collector', 'update_volume', job.to_hash(:host_device_name => linux_dev_path))
    puts "Attaching #{vol_id} on #{inst_id}"
    job.stm.on_attach
    job.on_attach

    # attach disk on guest os
    require 'net/telnet'
    slot_number = nil
    pci = nil

    telnet = ::Net::Telnet.new("Host" => "localhost", "Port"=>"#{inst[:runtime_config][:telnet_port]}", "Prompt" => /(qemu)/)
    telnet.cmd("pci_add auto storage file=#{linux_dev_path},if=scsi")
    telnet.waitfor(/(qemu)/){|c|
      pci_add = c.scan(/^(OK.+domain.+)/).last
      slot_number = pci_add.first.split(',')[2].split(' ').last if !pci_add.nil?
    }
    telnet.cmd("info pci"){|c|
      pci = c.scan(/^(.+[a-zA-z]+.+[0-9],.+device.+#{slot_number},.+:)/) 
    }
    telnet.close
    raise "volume has not attached" if pci.nil?
    rpc.request('sta-collector', 'update_volume', job.to_hash(:guest_device_name=>slot_number))
    puts "Attached #{vol_id} on #{inst_id}"
  end

  job :detach do
    inst_id = request.args[0]
    vol_id = request.args[1]

    job = Stm::VolumeContext.new(vol_id)
    inst = rpc.request('hva-collector', 'get_instance', inst_id)
    vol = rpc.request('sta-collector', 'get_volume', vol_id)
    puts "Detaching #{vol_id} on #{inst_id}"
    job.stm.state = vol[:state].to_sym
    raise "Invalid volume state: #{vol[:state]}" unless vol[:state].to_s == 'attached'

    job.stm.on_detach
    # detach disk on guest os
    require 'net/telnet'
    pci = nil

    telnet = ::Net::Telnet.new("Host" => "localhost", "Port"=>"#{inst[:runtime_config][:telnet_port]}", "Prompt" => /(qemu)/)
    telnet.cmd("pci_del #{vol[:guest_device_name]}")
    telnet.cmd("info pci"){|c|
      pci = c.scan(/^(.+[a-zA-z]+.+[0-9],.+device.+#{vol[:guest_device_name]},.+:)/) 
    }
    telnet.close
    raise "volume has not detached" unless pci.empty?
    rpc.request('sta-collector', 'update_volume', job.to_hash)

    # iscsi logout
    job.stm.on_detach
    job.on_detach
    puts "iscsi logout #{vol_id}: #{vol[:transport_information][:iqn]}"
    initiator = `sudo iscsiadm -m node -T '#{vol[:transport_information][:iqn]}' --logout`
    rpc.request('sta-collector', 'update_volume', job.to_hash)
  end

  def rpc
    @rpc ||= Isono::NodeModules::RpcChannel.new(@node)
  end

  def jobreq
    @jobreq ||= Isono::NodeModules::JobChannel.new(@node)
  end

  def event
    @event ||= Isono::NodeModules::EventChannel.new(@node)
  end


  class TimeoutError < RuntimeError; end
  
  def tryagain(opts={:timeout=>60, :retry=>3}, &blk)
    timedout = false
    curthread = Thread.current

    timersig = EventMachine.add_timer(opts[:timeout]) {
      timedout = true
      if curthread
        curthread.raise(TimeoutError.new("timeout"))
        curthread.pass
      end
    }

    begin
      count = 0
      begin
        break if blk.call
      end while !timedout && ((count += 1) < opts[:retry])
    rescue TimeoutError => e
      raise e
    ensure
      curthread = nil
      EventMachine.cancel_timer(timersig) rescue nil
    end
  end

end


manifest = DEFAULT_MANIFEST.dup
manifest.instance_eval do
  node_name 'hva'
  node_instance_id "#{Isono::Util.default_gw_ipaddr}"
  load_module Isono::NodeModules::NodeHeartbeat
  load_module ServiceNetfilter

  config do |c|
    c.vm_data_dir = '/var/lib/vm'
  end

  config_path File.expand_path('config/hva.conf', app_root)
  load_config
end

start(manifest) do
  endpoint "kvm-handle.#{@node.node_id}", KvmHandler
end
